---
date: "2021-03-14T23:03:01+08:00"
title: "Operating System: The Three Pieces"
authors: Nicholas Zhan
tags:
  - OS
categories:
  - 读书笔记
draft: false
toc: true
---
在我读过的写操作系统的书里，这应该是写得最好的了。本书的内容很好的体现了书名中“Three Easy Pieces”的思想，“Three Easy Pieces"即：虚拟化（virtualization）、并发（concurrency）和持久化（persistence）。

> Yeats famously said “Education is not the filling of a pail but the lighting of a fire.” His larger point is obviously on the mark: the real point of education is to get you interested in something, to learn something more about the subject matter on your own and not just what you have to digest to get a good grade in some class.

> And the author said: the real point of the educational process: to go forth, to study many new and fascinating topics, to learn, to mature, and most importantly, to find something that lights a fire for you.

* [Operating Systems: Three Easy Pieces](https://pages.cs.wisc.edu/~remzi/OSTEP/?source=techstories.org).


# 操作系统简介

程序运行时会发生什么？

一个正在运行的程序会做一件非常简单的事情：执行指令。CPU 从内存中取出（fetch）一条指令，对其进行译码（decode），然后执行（execute）它，保存执行结果，紧接着又去取指令，译码，执行指令……，如此周而复始，反复循环，使得计算机能够自动地工作。除非遇到停机指令（程序执行完成），否则这个循环将一直进行下去。

![程序的执行过程](/images/reading_notes/ostep/execution-process_of_a_procedure.jpg "程序的执行过程")

## 虚拟化

> 关键问题：如何将资源虚拟化？

操作系统负责确保既易使用又高效地运行，为此，它将物理资源（处理器、内存或磁盘）转换为更通用、更强大且更易于使用的虚拟形式，因此，我们有时也将操作系统称为虚拟机。为了让用户可以告诉操作系统要做什么，操作系统提供了一些 API（或者说系统调用）供程序调用。由于操作系统提供这些调用来运行程序、访问内存和设备、进行其它操作，所以我们有时也会说操作系统为应用程序提供了一个 **标准库（standard library）**。

操作系统早期被称为 **super visor** 或 **master control program**。因为虚拟化让许多程序运行（从而共享 CPU），让许多程序可以同时访问自己的指令和数据（从而共享内存），让许多程序访问设备（从而共享磁盘等），所以操作系统又被称为 **资源管理器（resource manager）**。每个 CPU、内存和磁盘都是系统的资源，因此操作系统扮演的主要角色就是管理这些资源，做到高效公平，或实现其它预定目标。

在硬件的帮助下，操作系统负责提供系统拥有非常多的虚拟 CPU 的假象。将单个 CPU（或其中一小部分）转换成为看似无限数量的 CPU，从而让许多程序看似在同时运行，这就是所谓的 **虚拟化 CPU**。

内存是一个字节数组。要读取内存，必须指定一个地址，才能访问存储在那里的数据。要写入或更新内存，还必须执行要向给定地址写入的数据。每个进程访问自己的私有虚拟地址空间（有时也直接称为地址空间），操作系统以某种方式将虚拟地址空间映射到机器的物理内存上去。一个正在运行的程序中的内存引用不会影响到其他进程（或操作系统本身）的地址空间。对于正在运行的程序，它完全拥有自己的物理内存。这就是 **虚拟化内存**。

## 并发

> 关键问题：如何构建正确的并发程序？

## 持久化

> 关键问题：如何持久化地存储数据？

在系统内存中，数据容易丢失，因为像 DRAM 这样的设备以易失（volatile）的方式存储数据。如果断电或系统崩溃，内存中的所有数据都会丢失。因此，我们需要硬件和软件来持久地存储数据。这样的存储对于所有系统都很重要，因为用户非常关心他们的数据。

操作系统中管理磁盘的软件通常为称为文件系统，它负责可靠高效地将用户创建的任何文件存储在系统的磁盘上。不想操作系统为 CPU 和内存提供的抽象，操作系统不会为每个应用程序创建专用的虚拟硬盘，相反，它假设用户经常需要 **共享** 文件中的信息。

## 操作系统的设计目标

操作系统取得 CPU、内存或磁盘等物理资源（resources），并对它们进行虚拟化（virtualize）。它处理与并发（concurrency）有关的麻烦事儿。它持久地（persistently）存储文件，从而使它们长期安全。我们希望建立这样一个系统，所以要有一些目标，以帮助我们集中设计和实现，并在必要时进行折中。找到合适的折中是建立系统的关键。以下是一些目标：

* 建立一些抽象，让系统方便易用。
* 提供高性能（最小化操作系统的开销）。
* 在应用程序之间以及在 OS 与应用程序之间提供保护。保护是操作系统和核心原理之一（即隔离），**让进程彼此隔离是保护的关键**。

## 参考资料

1. [CPU的工作过程](https://software.intel.com/content/www/cn/zh/develop/articles/book-processor-architecture_cpu_work_process.htm).

# CPU 虚拟化

## 进程

进程即运行中的程序。程序本身是没有生命周期的，它只是存在磁盘上的一些指令（也可能是一些静态数据）。事实表明，人们通常希望同时运行多个程序。

> 关键问题：如何提供有许多 CPU 的假象？

操作系统通过 CPU 虚拟化提供有许多 CPU 的假象，允许一个进程运行一段时间，然后切换到其它进程，这就是时分共享（time sharing）技术。时分共享的潜在缺点就是性能损失，因为如果 CPU 必须共享，那么每个进程的运行就会慢一点。

要实现好 CPU 虚拟化，操作系统需要一些底层机制（mechanism）和高级策略（policy）。底层机制是一些底层方法或协议，实现了所需的功能，比如上下文切换（context switch）。而高级策略则是在操作系统内做出某种决定的算法，比如调度策略。

操作系统为正在运行的程序提供的抽象，就是所谓的进程（process）。要理解构成进程的是什么，我们必须理解进程的机器状态（machine state）：程序在运行时可以读取或更新的内容。内存是进程的机器状态的一个重要组成部分，指令位于内存中，程序读写的数据也在内存中；进程的机器状态的另一部分是寄存器；由于程序经常访问持久存储设备，此类 I/O 信息可能包含当前打开的文件列表。

### 进程的创建

> 操作系统如何将程序转化为进程？

![](/images/reading_notes/ostep/from-program-to-process.png)

要将程序转化为进程，操作系统需要完成以下工作：

1. 将代码和所有静态数据加载到内存中，加载到进程的地址空间中。
2. 为程序的运行时栈分配一些内存。C 语言中的栈用于存放局部变量、函数参数和返回地址。
3. 操作系统也可能为程序的堆分配一些内存。C 语言中的堆用于显式请求的动态分配数据。
4. 执行其它初始化任务，特别是与 I/O 相关的任务。
5. 启动程序。OS 将 CPU 的控制转转交给新创建的进程，程序开始运行。

### 进程状态

早期的计算机系统中，进程可以处于以下三种状态之一：

* 运行（running）：进程正在处理器上运行，这意味着进程正在执行指令。
* 就绪（ready）。进程已经准备好，但由于某些原因，操作系统选择不在此时运行该进程。
* 阻塞（blocked）：进程等待特定事件发生时才会准备运行，比如等待 I/O 完成。

### 进程的数据结构

操作系统也是一个程序，和其它程序一样，它维护了一些关键的数据结构，用来跟踪各种相关的信息。例如，下面是 xv6 内核中的进程结构：

```c
// the registers xv6 will save and restore
// to stop and subsequently restart a process
struct context {
  int eip;
  int esp;
  int ebx;
  int ecx;
  int edx;
  int esi;
  int edi;
  int ebp;
};
// the different states a process can be in
enum proc_state { UNUSED, EMBRYO, SLEEPING,
                  RUNNABLE, RUNNING, ZOMBIE };

// the information xv6 tracks about each process
// including its register context and state
struct proc {
  char *mem;                  // Start of process memory
  uint sz;                    // Size of process memory
  char *kstack;               // Bottom of kernel stack for this process
  enum proc_state state;      // Process state
  int pid;                    // Process ID
  struct proc *parent;        // Parent process
  void *chan;                 // If !zero, sleeping on chan
  int killed;                 // If !zero, has been killed
  struct file *ofile[NOFILE]; // Open files
  struct inode *cwd;          // Current directory
  struct context context;     // Switch here to run process
  struct trapframe *tf;       // Trap frame for the current interrupt
};
```

## Limited Direct Execution

操作系统通过虚拟化 CPU 让多个任务共享物理 CPU，但是在构建这样的虚拟化时存在一些挑战：其一是性能，其二是控制权。控制权对于操作系统尤为重要，因为操作系统负责资源管理。操作系统应以高性能的方式虚拟化 CPU，同时保持对系统的控制。

> 关键问题：如何高效、可控地虚拟化 CPU？

受限直接执行（Limited Direct Execution）是操作系统开发人员想出的一种技术，目的是让程序尽可能快地运行。“受限”表示操作系统需要具备控制权，进程会受到一定的限制，“直接执行”就表示在 CPU 上运行程序。

### 受限制的操作

直接执行有一个明显的优势——快速，但是我们需要考虑当进程希望执行某种受限操作时（比如磁盘 I/O）该怎么办？

> 关键问题：如何执行受限制的操作？进程必须能够执行I/O等受限操作，但又不能完全让进程控制系统。

#### 用户模式与内核模式

操作系统与硬件协作，硬件提供不同的执行模式。在 **用户模式（user mode）** 下，应用程序不能访问全部的硬件资源，而在 **内核模式（kernel mode）** 下，操作系统可以访问机器的全部资源。

在用户模式下，运行的代码会受到限制。在内核模式下，运行的代码可以做任何事，操作系统（或内核）就运行在这个模式下。

系统调用允许内核向用户程序暴露某些关键功能，要执行系统调用，程序必须执行特殊的 **陷阱（trap）** 指令。该指令同时跳入内核并将特权级别提升到内核模式。系统调用执行完成后，操作系统调用一个特殊的 **从陷阱返回（return-from-trap）** 指令，返回到发起调用的用户程序中，同时将特权级别降低，回到用户模式。

在执行陷阱时，硬件必须确保保存足够的调用者寄存器，以便在操作系统发出从陷阱返回指令时能正确地返回。那么陷阱如何知道在 OS 内运行哪些代码呢？内核通过在启动时设置 **陷阱表（trap table）** 来呈现需要在 OS 内运行的代码。操作系统启动时做的有一件事情就是告诉硬件在发生某些异常事件时要运行哪些代码。

下面的时间线对首先直接运行协议进行了总结。整个过程假设每个进程都有一个内核栈，在进入内核时保存寄存器内容，在离开内核时恢复寄存器内容。

![LDE protocol](/images/reading_notes/ostep/lde-protocol.png)

LDE 协议有两个阶段。第一个阶段（在系统引导时），内核初始化陷阱表，并且 CPU 记住它的位置以供随后使用。内核通过特权指令来执行此操作（所有特权指令均以粗体突出显示）。第二个阶段（运行进程时），在使用从陷阱返回指令开始执行进程之前，内核设置了一些内容（例如，在进程列表中分配一个节点，分配内存）。这会将 CPU 切换到用户模式并开始运行该进程。当进程希望发出系统调用时，它会重新陷入操作系统，然后再次通过从陷阱返回，将控制权还给进程。该进程然后完成它的工作，并从 main()返回。这通常会返回到一些存根代码，它将正确退出该程序（例如，通过调用 exit()系统调用，这将陷入 OS 中）。此时，OS 清理干净，任务完成了。

### 进程切换

> 关键问题：操作系统如何重获 CPU 的控制权，以便它可以在进程间切换？

#### 协作方式

过去某些系统采用了一种被称为协作（cooperative）的方式，在这种方式下，操作系统相信进程会合理运行，运行时间过程的进程会定期放弃 CPU（通过系统调用），以便操作系统运行其它任务。

通常情况下，操作系统必须处理系统中的不当行为，如果某些进程执行了非法操作，操作系统需要重新控制 CPU。在协作方式中，某个进程若开启无限循环，并且不进行 yield 系统调用，那么操作系统则无法做任何事情。唯一的解决方式就是重启。

#### 非协作方式

通过 **时钟中断（timer interrupt）**，操作系统可以以非协作的方式获取 CPU 的控制权。时钟设备定时产生中断，产生中断时，当前运行的程序停止，操作系统预先配置的中断处理程序会运行。此时，操作系统重新获得 CPU 控制权，就可以做任何事情了，比如停止当前进程并启动另一个进程。

在发生中断时，硬件需要为当前正在运行的程序保存足够的状态，以便从陷阱返回指令能够正确恢复该程序。

**上下文切换（context switch）** ：操作系统要做的就是为当前正在执行的进程保存一些寄存器的值（例如，到它的内核栈），并为即将执行的进程恢复一些寄存器的值（从它的内核栈）。这样一来，操作系统就可以确保最后执行从陷阱返回指令时，不是返回到之前运行的进程，而是继续执行另一个进程。

下面是基于时钟的 LDE 协议：

![LDE protocol(time interrupt)](/images/reading_notes/ostep/lde-protocol-timer.png)

## 进程调度

> 关键问题：如何开发调度策略？什么是关键假设？哪些指标非常重要？

确定工作负载（workload）是构建调度策略的关键部分。工作负载了解得越多，你的调度策略就能越优化。此外，我们还需要一些调度指标，用来比较不同的调度策略。一个基本的调度指标是 **周转时间（turnround time）**。

任务的周转时间即任务完成时间减去任务到达系统的时间：

$$ T_{周转时间} = T_{完成时间} - T_{到达时间} $$

周转时间是一个性能指标。还有一个有趣的指标—— **公平性（fairness）**。性能和公平性在调度系统中往往是矛盾的。

### 先进先出

先进先出（First In First Out，FIFO）调度是一种基本的算法，有时候又称为先来先服务（First Come First Service，FCFS）。FIFO 的调度思想是：先运行最先达到系统的任务，然后是第二到达的，以此类推。

![](/images/reading_notes/ostep/fifo.png)

假设 A、B、C 三个任务几乎同时到达系统（A 比 B 早一点点，B 比 C 早一点点），若采用 FIFO 调度策略，则调度器会先执行 A，然后执行 B，最后执行 C。从图 FIFO 1 可以看出：A 在 10s 完成，B 在 20 秒完成，C 在 30 秒完成。那么这三个任务的平均周转时间就是 $\frac {10 + 20 + 30}{30} = 20 $。但是，若三个任务的运行情况如图 FIFO 2 所示的话，三个任务的平均周转时间就是 $\frac {100 + 110 + 120}{3} = 110$。

第二种情况通常被称为 **护航效应（convoy effect）**，即一些耗时短较少的潜在资源消耗者排在重量级的资源消费者之后。这对于耗时较短的任务来说，并不友好。

### 最短任务优先

最短任务优先（Shortest Job First， SJF）是另一种调度算法，解决了 FIFO 算法中耗时较短的任务等待时间可能过长的问题。SJF 的调度思想是：先运行最短的任务，然后是词短的任务，以此类推。

![](/images/reading_notes/ostep/sjf.png)

假设 A、B、C 同时到达（图 SJF 1），按照 SJF 原则，系统会先运行 A，然后运行 B，最后运行 C。因此，SJF 可以将 110s 的平均周转时间降低到 $\frac {10 + 20 + 120}{3} = 50$。事实上，当所有任务同时到达系统时，SJF 被证明是一个最优的调度算法。但是 SJF 算法也存在问题，假设 A 先到达，10s 之后 B、C 也到达（图 SJF 2）了，三个任务的平均周转时间则是 $\frac {100 + (110 - 10) + (120 - 10)}{3} = 103.33$，仍然出现了护航效应。

### 最短完成时间优先

SJF 调度算法是非抢占式（non-preemptive），而最短完成时间优先（Shortest Time-to-Completion First，STCF）或抢占式最短作业优先（Preemptive Shortest Job First，PSJF）是 SJF 的抢占式版本。每当新任务进入系统时，调度程序就会计算剩余任务和新任务之间，哪个任务的剩余时间最短，然后调度该任务。

![](/images/reading_notes/ostep/stcf.png)

还是上一个例子，采用 STCF 算法，平均周转时间大大提高：$\frac {120 + (20 - 10) + (30 - 10)}{3} = 50$

### 轮转

周转时间关心的是任务完成的快慢，适合批处理系统。而在分时系统中，用户就在系统前面，期望系统具有良好的交互性。因此，新的调度指标——**响应时间（response time）** 出现了。响应时间指的是从任务到达系统到系统首次运行它的时间，即

$$T_{响应时间} = T_{首次运行} - T_{到达时间} $$

在 STCF 的例子中，周转时间很好，但是响应时间就没那么好了，如果用户在系统响应前不得不等待 10s,那就太糟糕了！现在思考新的问题，如何构建对响应时间敏感的程序？

有一种简单的调度算法，叫做轮转（Round-Robin，RR）。它的基本思想很简单：RR 在一个时间片内运行一个任务，然后切换至运行队列中的下一个任务，反复执行，直到所有任务完成。时间片（time slice）有时又称调度量子（scheduling quantum），需要注意的是：**时间片的长度必须是时钟中断周期的整数倍**。

![](/images/reading_notes/ostep/rr.png)

假设 A、B、C 三个任务同时到达系统，并且都希望运行 10s。SJF 调度程序必须在运行完当前任务之后才可运行下一个任务（图 RR 1），而 1s 的时间片可以让 RR 调度程序调度各任务快速地循环工作。SJF 的平均响应时间为：$\frac {0 + 10 + 20}{3} = 10$，而 RR 的平均响应时间为：$\frac {0 + 1 + 2}{3} = 1$。

可以发现，时间片长度 对 RR 至关重要。时间片越短，RR 在响应时间上的表现就越好。然而，时间片也不是越短越好：频繁的上下文切换将影响系统整体性能。因此，系统设计者需要衡量时间片的长度，设置一个足够长的值，以便在摊销（amortize）上下文切换成本的同时保持系统的响应性。

### 多级反馈队列

多级反馈队列（Multi-level Feedback Queue，MLFQ）是一种应用于兼容时分共享系统（Compatible Time-Sharing System）的调度算法。它有两个目标：其一是优化周转时间，其二是给用户良好的交互体验（降低响应时间）。

> 关键问题：没有工作长度的先验知识，如何设计一个能同时减少响应时间和周转时间的调度程序。

方法是 **从历史中学习**。MLFQ 就是用历史经验预测未来的一个典型例子。

MLFQ 中有许多独立的队列，每个队列有不同的优先级。任何时刻，一个任务只能存在于一个队列中。MLFQ 总是优先执行较高优先级的任务（在较高级队列中的任务）。当然，一个队列中可能会有多个任务（这些任务的优先级相同），这时可以对它们采用轮转调度。因此，MLFQ 调度策略的关键在于 **如何设置优先级**。

#### 基本规则

MLFQ 的基本规则：

* 规则 1：If Priority(A) > Priority(B), A runs (B doesn't).
* 规则 2：If Priority(A) = Priority(B), A & B run in RR.
* 规则 3：When a job enters the system, it is placed at the highest priority (the topmost queue).
* 规则 4a：If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue).
* 规则 4b：If a job gives up the CPU before the time slice is up, it stays at the same priority level.
* 规则 5：After some time period S, move all the jobs in the system to the topmost queue.

规则 1 和 规则 2 是最基本的两条规则，给出了两个任务在给定优先级下的执行顺序，会出现低优先级的任务饥饿的情况。

规则 3、4a、4b 给出了改变优先级的规则。它能够根据负载情况调整任务的优先级，使得耗时任务之间可以公平地共享 CPU，并且短任务或交互性任务也能得到很好的响应时间，但是仍然存在饥饿的问题——过多的交互任务会导致耗时任务饥饿。

规则 5 定时改变优先级，消除了饥饿，保证了任务的执行。但是引入的时间段 S 又带来了新的问题——S 该如何设置？太长会让耗时任务会饥饿，太短会让交互型任务得不到合适的 CPU 时间。

一个更好的计时方式是修改规则 4a、4b。让调度程序记录一个进程在某一个队列中消耗的总时间，而不是在调度时重新计时。只要进程用完了自己的配额，就把它降到低一优先级的队列中去。因此，重写规则 4a、4b 为规则 4：

* 规则 4：Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).

#### 与其它调度策略的不同

MLFQ 不同于 FCFS、SJF、STCF，它不需要对任务的运作方式有先验知识，而是通过观察任务的运行情况来给出对应的优先级。

MLFQ 可以满足各种任务需求：对运行时间短的交互型任务，它可以获得类似于 SJF/STCF 的较不错的全局性能，同时对于耗时的 CPU 密集型负载也可以公平地稳步向前。

### 比例份额

比例份额（proportional-share）调度程序有时也称公平份额（fair-share）调度程序，其核心思想为：**调度程序的最终目标是确保每个任务获得一定比例的 CPU 时间，而不是优化周转时间和响应时间**。

#### 彩票调度

彩票调度（lottery scheduling）是比例份额调度程序的一个优秀例子。基本思想为：每隔一段时间就举行一次彩票抽奖，以确定接下来应该运行哪个进程。越是应该频繁运行的进程，越是应该拥有更多赢得彩票的机会。

**彩票数（ticket）** 是彩票调度背后的基本概念，它代表了进程占有某个资源的份额。一个资源拥有的彩票数占总彩票数的百分比，就是它占有的资源的份额。通过不断地定时抽取彩票，彩票调度从 **概率** 上获得这种份额比例。

彩票调度最精彩的地方在于利用了 **随机性**。随机性至少有三点优势：

1. 随机方法常常可以避免各种奇怪的边界情况。
2. 随机方法很轻量，几乎不需要记录任何状态。
3. 随机方法很快，只要能产生随机数，就能做出决策。

彩票调度还提供了一些机制，它们以不同的方式来调度彩票：

1. 彩票货币（ticket currency）允许拥有一组彩票的用户以自己喜欢的方式将彩票分给不同的任务。之后操作系统再自动将这种货币兑换为正确的全局彩票。
2. 彩票转让（ticket transfer）。通过转让，一个进程可以临时将自己的彩票交给另一个进程，加速它的执行。
3. 彩票膨胀（ticket inflation）。利用膨胀，一个进程可以临时提升或降低自己拥有的彩票数量。一般用于进程间相互信任的环境。

由于随机性，彩票调度在任务执行时间很短时，公平度非常差。只有当任务执行的时间片非常多时，才能得出想要的结果。这也是基于概率的算法的一个通病。

#### 步长调度

步长调度（stride scheduling）解决了彩票调度的不确定性问题，它是一个确定性的公平分配算法。在步长调度中，每个任务都有自己的步长，这个值与彩票数量成反比。

对于每一个进程，每次运行后，让进程的计数器（称为行程（pass）值）增加它的步长那么多的增量，记录进程执行的总体进度。之后，调度程序使用进程的步长及行程值来确定调度哪个进程。基本思想很简单：当需要进行调度时，选择目前行程值最小的进程，运行它并在运行完之后将该进程的行程值增加一个步长。

#### Linux 的完全公平调度器

Linux 中的完全公平调度器（Completely Fair Scheduler，CFS）不仅实现了公平份额调度，还具有非常高的效率和可扩展性。CFS 的目标是：在所有相互竞争的进程之间公平地均匀分配 CPU。Linux 是通过 **virtual time (vruntime)** 技术来实现的。

每个进程运行时都会累计 vruntime 值。最常见的是，每个进程的 vruntime 都以相同的速率累积。当需要进行调度时，CFS 会选取具有最小 vruntime 值的进程运行。这里有一个需要考虑的点——调度器如何知道该在何时停止当前运行的进程并运行下一个进程？如果 CFS 切换得过于频繁，公平性增加而性能降低（更多的上下文切换），反之则公平性降低而性能增加（更少的上下文切换）。

CFS 具体是通过一些控制参数来管理调度过程的。第一个参数是 sched_latency，CFS 用它来决定一个进程在切换前应当运行的时长，通常为 48ms。CFS 用进程的数量 n 除 sched_latency 得到单个进程的时间片，从而确保完全公平。但是，如果进程过多怎么办？时间片会因此变得非常小吗？上下文切换会更加频繁吗？并不会，CFS 为了解决这个问题，给了我们另一个参数 min_granularity，这个值通常为 6ms。CFS 永远不会将时间片的大小设置为小于这个值的数。

CFS 还支持控制进程的优先级，允许用户赋予某些进程更高的 CPU 份额。这并不是通过彩票数实现的，而是通过 Linux 中经典的进程 nice 值实现的。对于一个进程而言，其 nice 值可以是 [-20 , +19] 之间的任一整数，默认为 0。正数表示更低的优先级，而负数表示更高的优先级。CFS 会将 nice 值映射为权重：

```c
static const int prio_to_weight[40] = {
        /* -20 */ 88761, 71755, 56483, 46273, 36291,
        /* -15 */ 29154, 23254, 18705, 14949, 11916,
        /* -10 */ 9548, 7620, 6100, 4904, 3906,
        /* -5 */ 3121, 2501, 1991, 1586, 1277,
        /* 0 */ 1024, 820, 655, 526, 423,
        /* 5 */ 335, 272, 215, 172, 137,
         /* 10 */ 110, 87, 70, 56, 45,
        /* 15 */ 36, 29, 23, 18, 15,
};
```

有了权重之后，我们就可以计算出每个进程的时间片

$$ time \underline{\ } slice_k = \frac {weight_k}{\sum_{i=0}^{n-1}(weight_i)} \cdot sched \underline{\ } latency $$

此外，vruntime 的计算方式也有所改变：

$$ vruntime_i = vruntime_i + \frac {weight_0}{weight_i} \cdot runtime_i $$

### 多处理器调度

> 关键问题：操作系统应该如何在多 CPU 上调度任务？会遇到什么新问题？已有技术是否依然适用？

#### 缓存

多处理器与单 CPU 之间最大的区别在于 **硬件对缓存（cache）的使用，以及多处理器之间共享数据的方式**：

![](/images/reading_notes/ostep/cpu-with-cache.png)

缓存是基于 **局部性（locality）** 的。局部性分为两种：

* 时间局部性：当一个数据被访问后，她很有可能在不久的将来再次被访问。
* 空间局部性：当程序访问某个地址的数据时，很有可能会紧接着访问该地址周围的数据。

缓存可以有效地提升数据的访问速度，从而加快程序的运行速度。CPU 会先尝试从缓存中查找数据，若缓存中没有需要的数据，则访问内存。由于数据更新的存在，所以在在多处理器中，会出现 **缓存一致性（cache coherence）问题**。

通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性。在基于总线的系统中，一种方式就是使用总线窥探（bus snooping）。每个缓存都通过监听链接到所有缓存和内存的总线，来发现内存访问。如果 CPU 发现它放在缓存中的数据被更新了，就会作废（invalidate）本地的副本（从缓存中移除）或者更新它（修改会新的值）。

虽然缓存在一致性方面做了很多工作，但是应用程序还是需要关心共享数据的访问。跨 CPU 访问（尤其是写入）共享数据或数据结构时，需要使用 **互斥原语（比如锁）**，才能保证正确性。一些无锁的数据结构也能达到这个目的，但是它们非常复杂，使用得不是太多。同步操作对性能有影响，随着 CPU 数量的增加，访问同步共享的数据结构会变得很慢，因为需要做大量的同步操作。

多处理器调度中还需要考虑 **缓存亲和度（cache affinity）**：当进程在某个 CPU 上运行时，该 CPU 的缓存内会维护很多状态。若下次该进程还在同一个 CPU 上运行，它会由于缓存中的数据而执行得非常快。相反，在不同的 CPU 上执行同一个进程时，由于需要重新加载数据，所以会变得很慢。因此，最好是尽可能让同一个进程在同一个 CPU 上执行。

#### 多队列调度

有些系统采用了多队列的调度方案，比如每个 CPU 一个队列，称之为多队列多处理器调度（Multi-Queue Multiprocessor Scheduling，MQMS）。

在 MQMS 中，基本调度框架包含多个调度队列，每个队列可以使用不同的调度规则。当任务进入系统后，系统会按照一些启发性规则将其放入到某个调度队列。这样一来，每个 CPU 之间的调度相互独立，不需要同步。

MQMS 天生具有良好的缓存亲和度。所有的工作都保持在固定的 CPU 上，因而可以很好地利用缓存数据。但这又存在一个新的问题——**负载不均（load imbalance）**。

为了实现负载均衡。我们可以让任务跨 CPU 移动，这种技术被称为迁移（migration）。有很多种迁移模式，不过最棘手的部分就是如何决定发起迁移？有一个被称为 **工作窃取（work stealing）** 的技术，工作量较少的（源）队列不定期“偷看”其它（目标）队列的任务情况，如果目标队列更加拥挤，就从目标队列“窃取”一个或多个任务，实现负载均衡。在使用这种方法时，需要留意：对其它队列的检查不能太频繁，否则会带来较高的开销，但检查间隔又不能太长，否则可能带来严重的负载不均。所以需要找到合适的阈值。


#### Linux 上的多处理器调度

有趣的时，Linux 社区一直没有就构建多处理器调度成都达成共识。一直以来，存在三种不同的调度程序：

* O(1) 调度程序
* 完全公平调度程序（CFS）
* BF 调度程序（BFS）
  O(1) 和 CFS 采用多队列，而 BFS 采用单队列。

有关 BFS 的更多信息：http://ck.kolivas.org/patches/bfs/bfs-faq.txt

# 内存虚拟化

## 地址空间

操作系统为用户提供了一个易于使用的物理内存抽象，这个抽象叫做 **地址空间（address space）**。在系统中，地址空间是运行的程序看到的内存。

一个进程的地址空间包含运行的程序的所有内存状态。当程序运行时，利用栈（stack）保存当前的函数调用信息，分配空间给局部变量、传递参数和返回值。最后，堆（heap）用于管理动态分配的、由用户管理的内存。当然，还有其它的东西，比如静态初始化变量。不过我们现在可以假设地址空间只有三部分：代码、栈和堆。

![](/images/reading_notes/ostep/address-space-example.png)

上图展示了一个小型地址空间（只有 16KB）的例子。其中程序代码位于地址空间的顶部。在程序运行时，地址空间的堆（顶部）和栈（底部）两个区域可能动态增长或收缩。这种两端放置的方法只是一种约定，实际上可以按照任意方法放置。

当我们描述地址空间时，我们描述的是操作系统提供给运行程序的抽象。以上面 16KB 的地址空间为例，程序实际上可能处于物理内存中的任意位置，并不一定在 0-16KB之间。地址空间描述的是虚拟内存，它与物理内存是不同的。

> 关键问题：如何虚拟化内存？操作系统如何在单一的物理内存上为多个进程构建出一个私有的、可能无限大的地址空间的抽象？

隔离是建立可靠系统的关键原则。如果两个实体相互隔离，则一个实体的失败不会影星到另一个实体。操作系统尽力让进程之间彼此隔离，从而防止相互伤害。

## 虚拟化内存的目标

虚拟内存系统有几个重要的目标：

* 透明（transparency）：运行的程序并不会感知到内存被虚拟化的事实，程序的行为就好像它拥有自己的私有物理内存。背后是操作系统的辛勤工作，操作系统让不同的任务复用内存，从而提供这个假象。
* 效率（efficiency）：操作系统应该让虚拟化尽可能地高效。为此，可能需要像 TLB 这样的硬件支持。
* 保护（protection）：操作系统应当确保进程和自己受到保护，不受其它进程影响。保护让我们能够在进程之间提供隔离的特性，避免恶意进程的破坏。保护让我们能够在进程之间提供隔离机制，每个进程都应该在自己独立的环境中运行，避免其它出错或恶意进程的影响。

> C 程序里面可以打印指针，但是我们看到的值是虚拟地址而不是物理地址（我们看到的所有地址都不是真的）。虚拟地址只是提供地址如何在内存中分布的假象，只有操作系统（和硬件）知道物理地址。这也反映出了用户程序和操作系统看到东西的不同。

## 地址转换

前面提到过，运行中的程序看到的是虚拟地址，而数据是位于物理地址中的。为了读取正确的数据，需要进行虚拟地址到物理地址的转换。

基于硬件的地址转换（hardware-based address translation）简称地址转换（address translation），它是一种通用技术。每当程序进行内存访问（取指令、读写数据）时，地址转换将指令中的虚拟地址（virtual address）转换为实际的物理地址（physical address）。因此，在每次内存引用时，硬件都会进行地址转换，将应用程序的内存引用重定位到内存中的实际位置。

仅靠硬件不足以实现虚拟内存，硬件只是提供了底层机制来提高地址转换的效率，实现虚拟内存还需要操作系统在关键位置介入。因此，操作系统必须能够管理内存，记录内存的使用情况。

> **介入（interposiiton）** 是一种很常见又很有用的技术。在虚拟内存中，硬件可以介入到每次内存访问中，将进程提供的虚拟地址转换为数据实际所在的物理地址。

## 动态（基于硬件）重定位

基于硬件的地址转换在早期的时候很简单：每个 CPU 需要两个硬件寄存器——**基址（base）寄存器** 和 **界限（bound）寄存器**，界限寄存器有时又称限制（limit）寄存器。二者的合作能保证地址空间在物理内存中的任何位置，同时确保进程只能访问自己的地址空间。

当我们编写和编译程序时，假设地址空间从零开始。但是，当程序真正执行时，操作系统会决定其在物理内存中的实际地址，并将基址寄存器设置为这个值。当进程运行时，硬件会通过以下方式计算出物理地址并发给内存系统：

$$ physical\ address = virtual\ address + base $$

地址转换技术指的正是将虚拟地址转换为物理地址。硬件取得进程认为它要访问的地址，将其转换为数据实际位于的物理地址，由于这种重定位是在运行时发生的，所以这种技术一般被称为动态重定位（dynamic relocation）。

基址寄存器负责支持重定位，而界限寄存器就负责提供内存保护，确保进程产生的地址都在进程应该处于的界限之中。界限寄存器通常有两种工作方式：

* 记录地址空间的大小，在硬件将虚拟地址与基址寄存器内容求和前检查这个界限。
* 记录地址空间结束的物理地址，在硬件将虚拟地址转换成物理地址之后去检查这个界限。

下面是一个例子。假设一个进程拥有 4KB 大小的地址空间，它被加载到从 16KB 开始的物理内存中，一些地址转换结果如下表所示：

| 虚拟地址 |      | 物理地址     |
| -------- | ---- | ------------ |
| 0        | →    | 16KB         |
| 1KB      | →    | 17KB         |
| 3000     | →    | 19384        |
| 4400     | →    | 错误（越界） |

### 操作系统的职责

为了支持动态重定位，操作系统需要处理一些问题：

* 内存管理
  - 为新进程分配内存
  - 从终止进程回收内存
  - 管理内存使用情况
* 基址/界限管理
  - 必须在上下文切换时正确设置基址/界限寄存器
* 异常处理
  - 当异常发生时执行必要的处理逻辑

## 分段

如果简单地将进程的整个地址空间放入物理内存，栈和堆之间的空间即使没有被使用，也会占用实际的物理内存。因此，简单地通过基址寄存器和界限寄存器实现的虚拟内存非常浪费。

分段（segmentation）实际上是为了支持更大的地址空间。其思想是：让地址空间内的每个逻辑段都有一对基址和界限寄存器。一个段（segment）只是地址空间内的一个连续定长的区域。在典型的地址空间里有3种不同的逻辑段：代码段、栈和堆。分段可以让操作系统将不同的段放到不同的物理内存区域，从而避免虚拟地址空间中未使用的部分占用物理内存。

段错误是指在支持分段的机器上发生了非法的内存访问。

当硬件在地址转换时使用段寄存器时，它如何知道虚拟地址引用了那个段，以及段内偏移量呢？一种常见的方式是用虚拟地址的前几位来标识不同的段，剩余位用来标识偏移量。这种方式显式地给出段的位置，硬件可以通过隐式方式获知。例如，如果地址由程序计数器产生，那么地址位于代码段，如果地址基于栈或者基址指针，那么地址位于栈段，其它情况则位于堆段。

## 空闲空间管理

**外部碎片（external fragmentation）**：空闲空间被分割成不同大小的小块，成为碎片，后续的内存分配请求会由于找不到一块足够大的连续空闲空间而失败，即使这时总的空闲空间超出了请求的大小。
**内部碎片（internal fragmentation）**：如果分配程序分配给程序的内存块超过了请求的大小，那么超出大小的部分（未被使用）就会称为内部碎片，即空间的浪费出现在已分配单元的内部。

空闲链表（free list）是一种管理空闲空间的数据结构，该结构包含了所管理内存区域中所有空闲块的引用。

常见的内存分配策略有：最优匹配、最差匹配、首次匹配、下次匹配等

**分离空闲链表**：如果某个程序经常申请一种或集中大小的内存空间，那就用一个独立的列表来管理这些大小的对象，其它大小的请求交给通用的内存分配程序。例如 Solaris 的内存分配程序厚块分配程序（slab allocator）。


## 分页

在内存管理方面，操作系统通常有两种方法：

* 分段：将空间分割成不同长度的分片，就像虚拟内存管理中的分段。但是，将空间切成不同长度的碎片后，空间本身会碎片化，随着时间的推移，分配内存会变得比较困难。
* 分页：将空间分割成固定长度的分片。每个分片为一个单元，即一页，称之为页帧（page frame）。

### 页表

为了记录地址空间的每个虚拟页在物理内存中所处的位置，操作系统通常会为每个进程保存一个被称为页表（page table）的数据结构。页表的主要作用是为每个虚拟页面保存地址转换信息，从而让我们知道每个页在物理内存中的位置。

虚拟地址由两部分组成：虚拟页面号（virtual page number，VPN）和页内偏移量（offset）。

![](/images/reading_notes/ostep/virtual-address.png)

在进行地址转换时，先根据 VPN 检索页表，找出虚拟页号对应的物理帧号（PFN）或物理页号（PPN）。通过用 PFN 替换 VPN 转换虚拟地址，页内偏移保持不变（虚拟页和物理页大小相等），过程如下：

![](/images/reading_notes/ostep/address-translation-process.png)

#### 页表的保存

页表可以变得非常大，一个 20 位的 VPN 意味着操作系统必须为每个进程管理 1M （大约一百万）个地址转换。假设每个页表条目（PTE）占 4B，则每个页表就需要 4MB 的存储空间。实际上，操作系统将每个进程的页表存储在内存中，由于很多操作系统内存本身也可以被虚拟化，所以页表也可以存储在操作系统的虚拟内存中（甚至可以被交换到磁盘上）。

### 分页的问题

普通的分页存在两个问题：

* 页表太大，导致访问慢。
* 页表太大，导致消耗的内存太多。

### TLB

TLB 是为了解决分页速度慢的问题。

基于分页的虚拟内存可能会带来比较高的性能开销，因为需要将地址空间切分为固定大小的页，并记录这些页的地址映射信息。由于这些信息一般存储在物理内存中，所以在进行地址转换时，分页逻辑需要一次额外的内存访问。每次获取指令、加载或保存数据，都需要多读一次内存才能得到转换信息，这回使得总体的慢难以接受。为了加速地址转换，操作系统需要硬件的帮助，也就是所谓的 **地址转换旁路缓冲（translation-lookaside buffer，TLB）**。

对于每次内存访问，硬件都会先检查 TLB，若其中包含期望的地址转换，则不需要访问页表就能完成转换。页表中包含全部的转换映射，直接走 TLB 而不查询页表会很快，TLB 因此能够带来巨大的性能提升。

TLB 和其他缓存类似，前提都是在一般情况下，转换映射会在缓存中（即TLB命中）。若是如此，只需要增加少量开销就可以达到非常快的访问速度，因为 TLB 就在处理器附近。如果 TLB 未命中，就会产生很大的分页开销，必须访问页表获取转换映射，导致额外的内存访问。如果经常这样，程序的运行就会显著变慢。因此，我们希望尽可能地避免 TLB 不命中。

既然缓存这么快，为什么不把它做得更大呢？原因是如果想要快速地缓存，它就必须小，因为光速和其它物理限制会起作用。大的缓存注定慢，因此无法实现目的。所以我们应该关注如何利用好缓存来提高性能。

随机存取存储器（Random-Access Memory，RAM）暗示你访问 RAM 的任意部分都一样快。虽然 一般这样想 RAM 没错，但因为 TLB 这样的硬件/操作系统功能，访问某些内存页的开销较大，尤其是 没有被 TLB 缓存的页。因此，最好记住这个实现的窍门：**RAM 不总是 RAM**。有时候随机访问地址空 间，尤其是 TLB 没有缓存的页，可能导致严重的性能损失。

### 较小的页表

> 关键问题：简单的线性页表太大，占用过多内存，如何让它更小？

一种方法是使用更大的页，页的数量少了，页表因此变小。但是，更大的页容易出现内部碎片，造成内存的浪费。

另一种方法是混合使用分段和分页方法，每个分段一个页表。但这避不开分段可能导致的外部碎片问题。

还有一种方法是采用多级页表。多级页表是时间与空间的折中。

在反向页表中，只有一个页表，其中的项代表系统的每个物理页，而不是进程的页表。页表项告诉我们哪个进程正在使用此页，以及该进程的哪个虚拟页映射到了此物理页。

## 交换

> 关键问题：操作系统如何利用大而慢的设备，透明地提供巨大虚拟地址空间的假象？

### 交换空间

为了提供巨大虚拟地址空间的假象，我们可以在硬盘上开辟一部分空间（交换空间，swap space）用于物理页的移入和移出。在必要的时候将内存中的页面交换出去，在需要的时候又从其中交换回来。

需要注意的是，交换空间不是唯一的硬盘交换目的地。

### 交换时机

操作系统一般会预留一小部分空闲内存。大多数操作系统都会设置高水位线（High Watermark，HW）和低水位线（Low Watermark，LW），从而帮助决定何时从内存中清楚页。原理很简单：当操作系统发现有少于 LW 个页可用时，后台负责释放内存的线程就会开始运行，直到有 HW 个可用物理页。

### 页置换策略

内存其实只包含了系统中所有页的子集，所以我们可以将其视为系统中虚拟内存页的缓存。从磁盘获取页就是缓存未命中，从内存中找到待访问的页就是缓存命中。

最优替换策略：替换内存中在最远将来才会被访问到的页，这样能达到最低的缓存命中率。

FIFO：先入先出替换策略。

随机策略：在内存满的时候随机选择一个页进行替换。

LRU（Least-Recently-Used）：替换最近使用最少的页面。

LFU（Least-Frequently-Used）：替换最近嘴部经常使用的页面。


抖动（trashing）：当内存被超额请求时，操作系统将会不断地进行换页。

